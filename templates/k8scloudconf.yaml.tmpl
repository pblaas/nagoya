etcd:
  version:                     "{{ etcdver }}"
  advertise_client_urls:       "https://{{ ipaddress }}:2379"
  initial_advertise_peer_urls: "https://{{ ipaddress }}:2380"
  listen_client_urls:          "http://127.0.0.1:2379,https://{{ ipaddress }}:2379"
  listen_peer_urls:            "https://{{ ipaddress }}:2380"
  name:                        "infra{{ etcdid }}"
{% if isworker != 1 %}
  initial_cluster:             "{{ initialclusterlist }}"
  initial_cluster_token:       "etcd-cluster-{{ etcdtoken }}"
  initial_cluster_state:       "new"
{% endif %}
  cert_file:                   "/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem"
  key_file:                    "/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem"
  trusted_ca_file:             "/etc/ssl/certs/etcd-ca.pem"
  client_cert_auth:            true
  peer_cert_file:              "/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem"
  peer_key_file:               "/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem"
  peer_trusted_ca_file:        "/etc/ssl/certs/etcd-ca.pem"
  peer_client_cert_auth:       true
{% if isworker == 1 %}
  proxy: on
{% endif %}

networkd:
  units:
    - name: "00-eth.network"
      contents: |
        [Match]
        Name=eth0
        [Network]
        DNS={{ dnsserver }}
        Address={{ ipaddress }}
        Gateway={{ ipaddressgw }}

passwd:
  users:
    - name: core
      password_hash: {{ cryptedPass }}
      ssh_authorized_keys: [
        "{{ sshkey }}"
      ]
      groups:
        - "sudo"
        - "docker"

{% if netoverlay != "calico" %}
flannel:
  interface: {{ ipaddress }}
  version: "{{ flannelver }}"
  network_config: '{ "Network": "10.244.0.0/16", "Backend":{"Type":"vxlan"}}'
  etcd_endpoints: {{ etcdendpointsurls }}
  etcd_keyfile: /etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem
  etcd_certfile: /etc/ssl/certs/{{ ipaddress }}-etcd-node.pem
  etcd_cafile: /etc/ssl/certs/etcd-ca.pem
{% endif %}

systemd:
  units:
    - name: ip_vs.service
      enabled: true
      contents: |
        [Unit]
        Description=load ip_vs modules
        [Service]
        ExecStartPre=/sbin/modprobe ip_vs_rr
        ExecStartPre=/sbin/modprobe ip_vs_wrr
        ExecStartPre=/sbin/modprobe ip_vs_sh
        ExecStart=/sbin/modprobe ip_vs
        RemainAfterExit=yes
        Type=oneshot
        [Install]
        WantedBy=multi-user.target
{% if isworker == 1 %}
    - name: vm.max_map_count.service
      enabled: true
      contents: |
        [Unit]
        Description=Set vm.max_map_count
        [Service]
        ExecStart=/bin/sh -c "sysctl -w vm.max_map_count=384000"
        RemainAfterExit=yes
        Type=oneshot
        [Install]
        WantedBy=multi-user.target
    - name: increase-nf_conntrack-connections.service
      enabled: true
      contents: |
        [Unit]
        Description=Increase the number of connections in nf_conntrack.
        [Service]
        Type=idle
        ExecStartPre=/usr/sbin/modprobe nf_conntrack
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_max=589824"
        Restart=on-failure
        [Install]
        WantedBy=multi-user.target
    - name: increase-nf_conntrack-hashsize.service
      enabled: true
      contents: |
        [Unit]
        Description=Increase the nf_conntrack hashsize.
        [Service]
        Type=idle
        ExecStart=/bin/sh -c "echo 147456 > /sys/module/nf_conntrack/parameters/hashsize"
        Restart=on-failure
        [Install]
        WantedBy=multi-user.target
    - name: increase-port_range.service
      enabled: true
      contents: |
        [Unit]
        Description=Increase port_range.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range"
        [Install]
        WantedBy=multi-user.target
    - name: increase-net.core.somaxconn.service
      enabled: true
      contents: |
        [Unit]
        Description=Increase net.core.somaxconn.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.core.somaxconn=256"
        [Install]
        WantedBy=multi-user.target
    - name: change-conntrack_timeout.service
      enabled: true
      contents: |
        [Unit]
        Description=change conntrack tcp timeout.
        [Service]
        Type=idle
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=1"
        Restart=on-failure
        [Install]
        WantedBy=multi-user.target
    - name: change-tcp_timeout_estab.service
      enabled: true
      contents: |
        [Unit]
        Description=change tcp timeout estab.
        [Service]
        Type=idle
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600"
        Restart=on-failure
        [Install]
        WantedBy=multi-user.target
{% endif %}
    - name: settimezone.service
      enabled: true
      contents: |
        [Unit]
        Description=Set the time zone
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone Europe/Amsterdam
        RemainAfterExit=yes
        Type=oneshot
        [Install]
        WantedBy=multi-user.target
    - name: systemd-sysctl.service
      enabled: true
    - name: "ntpd.service"
      enabled: true
    - name: "docker.service"
      dropins:
        - name: "01.override-environment.conf"
          contents: |
            [Service]
            EnvironmentFile=-/etc/kubernetes/docker-environment.env
    - name: kubelet.service
      enabled: true
      contents: |
        [Unit]
        Requires=docker.service
        After=docker.service
        [Service]
        Environment=RKT_GLOBAL_ARGS="--insecure-options=image"
        Environment=KUBELET_IMAGE_URL=docker://gcr.io/google-containers/hyperkube-amd64
        Environment=KUBELET_IMAGE_TAG={{ k8sver }}
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume coreos-tmp,kind=host,source=/tmp \
        --mount volume=coreos-tmp,target=/tmp \
{% if netoverlay == "calico" %}
        --volume cni-net,kind=host,source=/etc/cni/net.d \
        --mount volume=cni-net,target=/etc/cni/net.d \
        --volume cni-bin,kind=host,source=/opt/cni/bin \
        --mount volume=cni-bin,target=/opt/cni/bin \
        --volume cni-varlib,kind=host,source=/var/lib/calico \
        --mount volume=cni-varlib,target=/var/lib/calico \
{% endif %}
        --hosts-entry=host"
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /tmp/k8stmp
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStartPre=/bin/mkdir -p /var/lib/kubelet/volumeplugins
        ExecStartPre=/bin/mkdir -p /var/lib/rook
{% if netoverlay == "calico" %}
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/cni/net.d
        ExecStartPre=/usr/bin/mkdir -p /var/lib/calico
{% endif %}
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml \
{% if isworker != 1 %}
        --node-labels=node-role.kubernetes.io/master=true \
        --register-with-taints=node-role.kubernetes.io/master=true:NoSchedule \
{% else %}
        --register-node=true \
        --node-labels=node-role.kubernetes.io/worker=true \
{% endif %}
{% if netoverlay == "calico" %}
        --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
{% else %}
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
{% endif %}
        --network-plugin=cni \
        --container-runtime=docker \
        --allow-privileged=true \
        --config=/etc/kubernetes/kubelet.config \
{% if cloudprovider == "external" %}
        --cloud-provider=external \
{% endif %}
{% if cloudprovider == "openstack" %}
        --cloud-provider=openstack \
        --cloud-config=/etc/kubernetes/cloud.conf \
{% endif %}
        --volume-plugin-dir=/var/lib/kubelet/volumeplugins
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target

locksmith:
  reboot_strategy: "etcd-lock"
  etcd_endpoints:  {{ etcdendpointsurls }}
  etcd_keyfile: /etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem
  etcd_certfile: /etc/ssl/certs/{{ ipaddress }}-etcd-node.pem
  etcd_cafile: /etc/ssl/certs/etcd-ca.pem
  window_start: "Thu 04:00"
  window_length: "1h"

storage:
  files:
    - filesystem: "root"
      path:       "/etc/hostname"
      mode:       0644
      contents:
        inline: k8s-{{clustername}}-node{{ node }}
    - filesystem: "root"
      path: "/etc/resolv.conf"
      mode: 0644
      contents:
        inline: |
          nameserver {{ dnsserver }}
    - filesystem: "root"
      path: "/etc/motd.d/k8s.conf"
      mode: 0644
      contents:
        local: "../motd.conf"
    - filesystem: "root"
      path:       "/etc/kubernetes/docker-environment.env"
      mode:       0644
      contents:
        inline: |
          DOCKER_OPT_BIP="--bip=172.17.0.1/24"
          DOCKER_OPT_IPMASQ="--ip-masq=false"
          DOCKER_OPT_MTU="--mtu=1450"
    - filesystem: "root"
      path: "/etc/kubernetes/cni/docker_opts_cni.env"
      mode: 0664
      contents:
        inline: |
          DOCKER_OPT_BIP=""
          DOCKER_OPT_IPMASQ=""
    - filesystem: "root"
      path: "/etc/kubernetes/cni/net.d/10-flannel.conf"
      mode: 0644
      contents:
        inline: |
          {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
              "isDefaultGateway": true,
              "hairpinMode": true
            }
          }
    - filesystem: "root"
      path: "/home/core/etcdinfo.sh"
      mode: 0665
      contents:
        inline: |
          /usr/bin/etcdctl --cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem --key-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem --ca-file=/etc/kubernetes/ssl/etcd-ca.pem  cluster-health
{% if isworker == 1 %}
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-k8s-node-key.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-k8s-node.pem"
{% endif %}
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/ca.pem"
      mode: 0664
      contents:
        local: "ca.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-etcd-node-key.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-etcd-node.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/etcd-ca.pem"
      mode: 0664
      contents:
        local: "etcd-ca.pem"
    - filesystem: "root"
      path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-etcd-node-key.pem"
    - filesystem: "root"
      path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem"
      mode: 0664
      contents:
        local: "{{ ipaddress }}-etcd-node.pem"
    - filesystem: "root"
      path: "/etc/ssl/certs/etcd-ca.pem"
      mode: 0664
      contents:
        local: "etcd-ca.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/cloud.conf"
      mode: 0664
      contents:
        local: "../cloud.conf"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/cloud.conf"
      mode: 0664
      contents:
        local: "../cloud.conf"
    - filesystem: "root"
      path: "/etc/kubernetes/kubelet.config"
      mode: 0664
      contents:
        local: "../kubelet.config"
    - filesystem: "root"
      path: "/etc/kubernetes/kubeproxy.config"
      mode: 0664
      contents:
        local: "../kubeproxy.config"
    - filesystem: "root"
      path: "/etc/kubernetes/kubeproxy-config.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
{% if isworker == 1 %}
              server: https://{{ loadbalancer }}
{% else %}
              server: https://{{ ipaddress }}
{% endif %}
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kubeproxy
            user:
              client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-proxy.pem
              client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-proxy-key.pem
          contexts:
          - context:
              cluster: local
              user: kubeproxy
            name: kubeproxy-context
          current-context: kubeproxy-context
    - filesystem: "root"
      path: "/etc/kubernetes/master-kubeconfig.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
{% if isworker == 1 %}
              server: https://{{ loadbalancer }}
{% else %}
              server: https://{{ ipaddress }}
{% endif %}
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kubelet
            user:
              client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
              client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
          contexts:
          - context:
              cluster: local
              user: kubelet
            name: kubelet-context
          current-context: kubelet-context
{% if isworker != 1 %}
    - filesystem: "root"
      path: "/etc/kubernetes/controller-manager.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
              server: https://{{ ipaddress }}
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kube-controller-manager
            user:
              client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-cm.pem
              client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-cm-key.pem
          contexts:
          - context:
              cluster: local
              user: kube-controller-manager
            name: kube-controller-manager-context
          current-context: kube-controller-manager-context
    - filesystem: "root"
      path: "/etc/kubernetes/scheduler.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
              server: https://{{ ipaddress }}
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kube-scheduler
            user:
              client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-scheduler.pem
              client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-kube-scheduler-key.pem
          contexts:
          - context:
              cluster: local
              user: kube-scheduler
            name: kube-scheduler-context
          current-context: kube-scheduler-context
{% endif %}
    - filesystem: "root"
      path: /etc/ntp.conf
      mode: 0644
      contents:
        inline: |
          server 0.europe.pool.ntp.org
          server 1.europe.pool.ntp.org
          server 2.europe.pool.ntp.org
          restrict default nomodify nopeer noquery limited kod
          restrict 127.0.0.1
          restrict [::1]
{% if isworker != 1 %}
    - filesystem: "root"
      path: "/etc/kubernetes/manifests/kube-apiserver.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-apiserver
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-apiserver
              image: gcr.io/google-containers/hyperkube-amd64:{{ k8sver }}
              command:
              - /hyperkube
              - apiserver
              - --bind-address=0.0.0.0
              - --etcd-servers={{ etcdendpointsurls }}
              - --etcd-cafile=/etc/kubernetes/ssl/etcd-ca.pem
              - --etcd-certfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
              - --etcd-keyfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
              - --apiserver-count={{ managers }}
              - --allow-privileged=true
              - --storage-backend=etcd3
              - --service-cluster-ip-range=10.3.0.0/24
              - --secure-port=443
              - --advertise-address={{ ipaddress }}
{% if alphafeatures == "true" %}
              - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota,NodeRestriction
              - --runtime-config=extensions/v1beta1/networkpolicies=true
{% else %}
              - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota,NodeRestriction
              - --runtime-config=extensions/v1beta1/networkpolicies=true
{% endif %}
              - --tls-cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
              - --tls-private-key-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
              - --client-ca-file=/etc/kubernetes/ssl/ca.pem
              - --service-account-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem
              - --kubelet-client-certificate=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-kubelet-client.pem
              - --kubelet-client-key=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-kubelet-client-key.pem
              - --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-client-ca.pem
              - --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem
              - --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem
              - --requestheader-allowed-names=front-proxy-client
              - --requestheader-extra-headers-prefix=X-Remote-Extra-
              - --requestheader-group-headers=X-Remote-Group
              - --requestheader-username-headers=X-Remote-User
              - --anonymous-auth=false
{% if cloudprovider == "openstack" %}
              - --cloud-provider=openstack
              - --cloud-config=/etc/kubernetes/ssl/cloud.conf
{% endif %}
{% if rbac == "true" %}
              - --authorization-mode=Node,RBAC
{% else %}
              - --authorization-mode=AlwaysAllow
{% endif %}
              - --v={{ apidebuglevel }}
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  port: 8080
                  path: /healthz
                initialDelaySeconds: 15
                timeoutSeconds: 15
              ports:
              - containerPort: 443
                hostPort: 443
                name: https
              - containerPort: 8080
                hostPort: 8080
                name: local
              volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
{% endif %}
    - filesystem: "root"
      path: "/etc/kubernetes/manifests/kube-proxy.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-proxy
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-proxy
              image: gcr.io/google-containers/hyperkube-amd64:{{ k8sver }}
              command:
              - /hyperkube
              - proxy
              - --config=/etc/kubernetes/kubeproxy.config
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
              - mountPath: /etc/kubernetes/kubeproxy-config.yaml
                name: "kubeconfig"
                readOnly: true
              - mountPath: /etc/kubernetes/kubeproxy.config
                name: "kubeproxy-config"
                readOnly: true
              - mountPath: /etc/kubernetes/ssl
                name: "etc-kube-ssl"
                readOnly: true
            volumes:
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
            - name: "kubeconfig"
              hostPath:
                path: "/etc/kubernetes/kubeproxy-config.yaml"
            - name: "etc-kube-ssl"
              hostPath:
                path: "/etc/kubernetes/ssl"
            - name: "kubeproxy-config"
              hostPath:
                path: "/etc/kubernetes/kubeproxy.config"
{% if isworker != 1 %}
    - filesystem: "root"
      path: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-controller-manager
              image: gcr.io/google-containers/hyperkube-amd64:{{ k8sver }}
              command:
              - /hyperkube
              - controller-manager
              - --master=http://127.0.0.1:8080
              - --leader-elect=true
              - --service-account-private-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem
              - --root-ca-file=/etc/kubernetes/ssl/ca.pem
              - --kubeconfig=/etc/kubernetes/controller-manager.yaml
              - --authentication-kubeconfig=/etc/kubernetes/controller-manager.yaml
              - --authorization-kubeconfig=/etc/kubernetes/controller-manager.yaml
              - --v={{ apidebuglevel }}
              - --node-monitor-grace-period=120s
              - --node-monitor-period=10s
{% if cloudprovider == "external" %}
              - --cloud-provider=external
{% endif %}
{% if cloudprovider == "openstack" %}
              - --cloud-provider=openstack
              - --cloud-config=/etc/kubernetes/ssl/cloud.conf
{% endif %}
              resources:
                requests:
                  cpu: 200m
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10252
                initialDelaySeconds: 15
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/kubernetes/controller-manager.yaml
                name: kubeconfig
                readOnly: true
              - mountPath: /etc/kubernetes/ssl
                name: etc-kube-ssl
                readOnly: true
            volumes:
            - name: kubeconfig
              hostPath:
                path: /etc/kubernetes/controller-manager.yaml
            - name: etc-kube-ssl
              hostPath:
                path: /etc/kubernetes/ssl
    - filesystem: "root"
      path: "/etc/kubernetes/manifests/kube-scheduler.yaml"
      mode: 0644
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-scheduler
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-scheduler
              image: gcr.io/google-containers/hyperkube-amd64:{{ k8sver }}
              command:
              - /hyperkube
              - scheduler
              - --master=http://127.0.0.1:8080
              - --leader-elect=true
              - --kubeconfig=/etc/kubernetes/scheduler.yaml
              - --authentication-kubeconfig=/etc/kubernetes/scheduler.yaml
              - --authorization-kubeconfig=/etc/kubernetes/scheduler.yaml              
              - --v={{ apidebuglevel }}
              resources:
                requests:
                  cpu: 100m
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10251
                initialDelaySeconds: 15
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
              - mountPath: /etc/kubernetes/scheduler.yaml
                name: "kubeconfig"
                readOnly: true
              - mountPath: /etc/kubernetes/ssl
                name: "etc-kube-ssl"
                readOnly: true
            volumes:
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
            - name: "kubeconfig"
              hostPath:
                path: "/etc/kubernetes/scheduler.yaml"
            - name: "etc-kube-ssl"
              hostPath:
                path: "/etc/kubernetes/ssl"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem"
      mode: 0664
      contents:
        local: "sa-{{ clustername }}-k8s-key.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem"
      mode: 0664
      contents:
        local: "sa-{{ clustername }}-k8s.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/front-proxy-client-key.pem"
      mode: 0664
      contents:
        local: "front-proxy-client-key.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/front-proxy-client.pem"
      mode: 0664
      contents:
        local: "front-proxy-client.pem"
    - filesystem: "root"
      path: "/etc/kubernetes/ssl/front-proxy-client-ca.pem"
      mode: 0664
      contents:
        local: "front-proxy-client-ca.pem"
{% endif %}
