#cloud-config

hostname: "{{ ipaddress }}"
coreos:
  flannel:
    interface: {{ ipaddress }}
  etcd2:
    discovery: {{ discoveryid }}
    advertise-client-urls: https://{{ ipaddress }}:2379
    initial-advertise-peer-urls: https://{{ ipaddress }}:2380
    listen-client-urls: http://127.0.0.1:2379,https://{{ ipaddress }}:2379
    listen-peer-urls: https://{{ ipaddress }}:2380
    cert-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
    key-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
    trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    client-cert-auth: true
    peer-cert-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
    peer-key-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
    peer-trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    peer-client-cert-auth: true
{% if isworker == 1 %}
    proxy: on
{% endif %}
  fleet:
    metadata: "role=node"
  units:
    - name: etcd2.service
      command: stop
    - name: fleet.service
      command: stop
    - name: iptables-restore.service
      command: stop
    - name: 00-enp0s3.network
      runtime: true
      content: |
        [Match]
        Name=ens32
        [Network]
        DNS={{ dnsserver }}
        Address={{ ipaddress }}
        Gateway={{ ipaddressgw }}
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service
            [Service]
{% if isworker != 1 %}
            ExecStartPre=/usr/bin/etcdctl --cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem --key-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem --ca-file=/etc/kubernetes/ssl/etcd-ca.pem set /coreos.com/network/config '{ "Network": "10.244.0.0/16", "Backend":{"Type":"vxlan"}}'
{% endif %}
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      command: start
    - name: kubelet.service
      content: |
        [Unit]
        Requires=docker.service
        After=docker.service
        [Service]
        Environment=KUBELET_IMAGE_TAG={{ k8sver }}
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
{% if netoverlay == "calico" %}
        --volume cni-net,kind=host,source=/etc/cni/net.d \
        --mount volume=cni-net,target=/etc/cni/net.d \
        --volume cni-bin,kind=host,source=/opt/cni/bin \
        --mount volume=cni-bin,target=/opt/cni/bin \
{% endif %}
        --hosts-entry=host"
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
{% if netoverlay == "calico" %}
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/cni/net.d
{% endif %}
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml \
        --require-kubeconfig \
{% if isworker != 1 %}
        --register-schedulable=false \
{% else %}
        --register-node=true \
{% endif %}
{% if netoverlay == "calico" %}
        --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
{% else %}
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
{% endif %}
        --network-plugin=cni \
        --container-runtime=docker \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override={{ ipaddress }} \
        --cluster_dns=10.3.0.10 \
        --cluster_domain=cluster.local \
        --client-ca-file=/etc/kubernetes/ssl/ca.pem \
        --anonymous-auth=false \
        --cloud-provider={{ cloudprovider }} \
        --cloud-config=/etc/kubernetes/cloud.conf
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
      command: start
    - name: increase-nf_conntrack-connections.service
      command: start
      content: |
        [Unit]
        Description=Increase the number of connections in nf_conntrack.
        [Service]
        Type=oneshot
        ExecStartPre=/usr/sbin/modprobe nf_conntrack
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_max=589824"
    - name: increase-nf_conntrack-hashsize.service
      command: start
      content: |
        [Unit]
        Description=Increase the nf_conntrack hashsize.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 147456 > /sys/module/nf_conntrack/parameters/hashsize"
    - name: increase-port_range.service
      command: start
      content: |
        [Unit]
        Description=Increase port_range.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range"
    - name: increase-net.core.somaxconn.service
      command: start
      content: |
        [Unit]
        Description=Increase net.core.somaxconn.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.core.somaxconn=256"
    - name: change-conntrack_timeout.service
      command: start
      content: |
        [Unit]
        Description=change conntrack tcp timeout.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=1"
    - name: change-tcp_timeout_estab.service
      command: start
      content: |
        [Unit]
        Description=change tcp timeout estab.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600"
    - name: settimezone.service
      command: start
      content: |
        [Unit]
        Description=Set the time zone
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone Europe/Amsterdam
        RemainAfterExit=yes
        Type=oneshot
    - name: vm.max_map_count.service
      command: start
      content: |
        [Unit]
        Description=Set vm.max_map_count
        [Service]
        ExecStart=/bin/sh -c "sysctl -w vm.max_map_count=384000"
        RemainAfterExit=yes
        Type=oneshot        
    - name: systemd-timesyncd.service
      command: stop
      mask: true
    - name: ntpd.service
      command: start
      enable: true
    - name: systemd-modules-load.service
      command: restart
    - name: systemd-sysctl.service
      command: restart
  update:
    reboot-strategy: "etcd-lock"
  locksmith:
    window-start: Thu 04:00
    window-length: 1h
users:
  - name: "core"
    passwd: "USER_CORE_PASSWORD"
    groups:
      - "sudo"
      - "docker"
write_files:
  - path: "/home/core/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD
  - path: "/root/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD:/home/core
  - path: "/var/lib/iptables/rules-save"
    permissions: "0644"
    owner: "root"
    content: |
      *filter
      :INPUT DROP [0:0]
      :FORWARD DROP [0:0]
      :OUTPUT ACCEPT [0:0]
      -A INPUT -i lo -j ACCEPT
      -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2379 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2380 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8472 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8285 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT
      COMMIT
  - path: "/etc/resolv.conf"
    permissions: "0644"
    owner: "root"
    content: |
      nameserver {{ dnsserver }}
  - path: "/home/core/getkube.sh"
    permissions: "0644"
    owner: "core"
    content: |
     #!/bin/bash
     curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
     chmod +x kubectl
  - path: "/etc/kubernetes/cni/docker_opts_cni.env"
    permissions: "0644"
    owner: "root"
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
  - path: "/etc/kubernetes/cni/net.d/10-flannel.conf"
    permissions: "0644"
    owner: "root"
    content: |
      {
        "name": "podnet",
        "type": "flannel",
        "delegate": {
          "isDefaultGateway": true
        }
      }
  - path: "/etc/flannel/options.env"
    permissions: "0644"
    owner: "root"
    content: |
      FLANNELD_IFACE={{ ipaddress }}
      FLANNELD_ETCD_ENDPOINTS={{ etcdendpointsurls }}
      FLANNELD_ETCD_KEYFILE=/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem
      FLANNELD_ETCD_CERTFILE=/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem
      FLANNELD_ETCD_CAFILE=/etc/ssl/certs/etcd-ca.pem
      FLANNEL_IMAGE_TAG={{ flannelver }}
{% if isworker != 1 %}
  - path: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers={{ etcdendpointsurls }}
          - --etcd-cafile=/etc/kubernetes/ssl/etcd-ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
          - --apiserver-count={{ managers }}
          - --allow-privileged=true
          - --storage-backend=etcd2
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --advertise-address={{ ipaddress }}
{% if alphafeatures == "true" %}
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction,Initializers
          - --runtime-config=extensions/v1beta1/networkpolicies=true,admissionregistration.k8s.io/v1alpha1
{% else %}
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction
          - --runtime-config=extensions/v1beta1/networkpolicies=true
{% endif %}
          - --tls-cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem
          - --kubelet-client-certificate=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          - --kubelet-client-key=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
          - --anonymous-auth=false
          - --authorization-mode={{ authmode }}
          - --v=1
          - --cloud-provider={{ cloudprovider }}
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
{% endif %}
  - path: "/etc/kubernetes/manifests/kube-proxy.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - proxy
{% if isworker == 1 %}
          - --master=https://{{ loadbalancer }}
{% else %}
          - --master=http://127.0.0.1:8080
{% endif %}
          - --cluster-cidr=10.244.0.0/16
          - --conntrack-max-per-core=0
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/master-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
{% if isworker != 1 %}
  - path: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider={{ cloudprovider }}
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: kubeconfig
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: etc-kube-ssl
            readOnly: true
        volumes:
        - name: kubeconfig
          hostPath:
            path: /etc/kubernetes/master-kubeconfig.yaml
        - name: etc-kube-ssl
          hostPath:
            path: /etc/kubernetes/ssl
  - path: "/etc/kubernetes/manifests/kube-scheduler.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/master-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
  - path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ sak8skeybase64 }}
  - path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ sak8sbase64 }}
{% endif %}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ k8snodekeybase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ k8snodebase64 }}
  - path: "/etc/kubernetes/ssl/ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cabase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem"
    permissions: "0644"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodekeybase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodebase64 }}
  - path: "/etc/kubernetes/ssl/etcd-ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdcabase64 }}
  - path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem"
    permissions: "0644"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodekeybase64 }}
  - path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodebase64 }}
  - path: "/etc/ssl/certs/etcd-ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdcabase64 }}
  - path: "/etc/kubernetes/cloud.conf"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cloudconfbase64 }}
  - path: "/etc/kubernetes/ssl/cloud.conf"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cloudconfbase64 }}
  - path: /etc/motd.d/k8s.conf
    owner: "root"
    permissions: "0644"
    encoding: "gzip+base64"
    content: |
      H4sIAJ7IHVkAA3WTO7rEIAiF+6zCng/oWQ07oKDK7u9Bx6iTuTTx8R8CAq19mzOLv05h1/eBsEbed2YYvxQ77WKUIKdBoafi+kkSP8tSyEmfPu/b2rGFwhfN5xWRNKU4BLHo6HuxvJPY3aUiw0ejDqzjPGn++DRBTkAyERt+YRAIUy7n13RdvDTQuC261BUszVhk0DL33CRUqjaIxGoR2nz6okFPdfZlasVWz8zRPdrm/Go+c0dkpFaP61EalECJ16+1aO1+FVzVXZitQCwhrG5xg6gzDjo/dCDIHhUViXSrWyqVoEEjr6utIt/qbhFeR+R4RFRJZpJ4+fLdXB8+qjYgqp/wBzz+djVruRygov5YW2yORhld5TbPzQl1gWUmP15Czo6VkUjyUzts7HNo72ngo+c2Da2B2GZny3Zje3a/Jg0z+TLZ799T/K9df0AUN+scBAAA
  - path: "/etc/kubernetes/master-kubeconfig.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
{% if isworker == 1 %}
          server: https://{{ loadbalancer }}
{% else %}
          server: https://{{ ipaddress }}
{% endif %}
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
  - path: /etc/ntp.conf
    content: |
      server 0.europe.pool.ntp.org
      server 1.europe.pool.ntp.org
      server 2.europe.pool.ntp.org
      # - Allow only time queries, at a limited rate.
      # - Allow all local queries (IPv4, IPv6)
      restrict default nomodify nopeer noquery limited kod
      restrict 127.0.0.1
      restrict [::1]
